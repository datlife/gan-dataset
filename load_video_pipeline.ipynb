{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH ='/media/dat/dataset/YOUTUBE_BB/youtube_boundingboxes_detection_validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import get_video_ids\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Get all video ids that satisfies the condition as stated \n",
    "grouped = get_video_ids(PATH, \n",
    "                        category='car', \n",
    "                        min_area=0.08, \n",
    "                        max_area=0.4, \n",
    "                        threshold=0.7)\n",
    "\n",
    "print(\"Number of videos: %s\" % len(grouped))\n",
    "\n",
    "# Display an example\n",
    "idx = np.random.randint(0, len(grouped.groups.keys()))\n",
    "video_id = list(grouped.groups.keys())[idx]\n",
    "\n",
    "# Download youtube video and save into /tmp/\n",
    "YouTubeVideo(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pytube import YouTube\n",
    "\n",
    "# Reference\n",
    "# https://docs.opencv.org/3.3.0/d4/d15/group__videoio__flags__base.html\n",
    "\n",
    "# Download video into /tmp\n",
    "yt = YouTube('https://www.youtube.com/watch?v=%s' % video_id)\n",
    "title  = yt.title\n",
    "stream = yt.streams.filter(file_extension='mp4').first().download('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "import  matplotlib.pyplot as plt\n",
    "tqdm\n",
    "class VideoAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Look for fixed-angle video sequences in a video\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model        , \n",
    "                 data_transform ,\n",
    "                 metric       = '',\n",
    "                 stabilizer   = None):\n",
    "\n",
    "        self.model      = model\n",
    "        self.data_transform = data_transform\n",
    "        self.metric     = metric\n",
    "        self.stabilizer = stabilizer\n",
    "\n",
    "    def process(self, video_path, sequence_len=50):\n",
    "        \n",
    "        print(os.path.isfile(video_path))\n",
    "        \n",
    "        cap    = cv2.VideoCapture(video_path)\n",
    "        # Init video params (video length, width, height, fps)\n",
    "        if cap.isOpened():\n",
    "            param = self._set_video_params(cap)\n",
    "        print(cap.isOpened())\n",
    "        frame_size = (param['video_height'],param['video_width'],  3)\n",
    "        fig = plt.figure(figsize=(8,3))\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "        fig.tight_layout()\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        b2 = fig.canvas.copy_from_bbox(ax2.bbox)\n",
    "        im1 =  ax1.imshow(np.random.randint(0, 255, frame_size))\n",
    "        im2 =  ax2.imshow(np.random.randint(0, 255, frame_size))\n",
    "        \n",
    "        for curr_cursor in range(0 , param['video_length'], sequence_len):\n",
    "            first_frame_idx =  curr_cursor \n",
    "            last_frame_idx  =  curr_cursor + sequence_len\n",
    "            \n",
    "            if last_frame_idx > param['video_length']:\n",
    "                break\n",
    "                \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,  first_frame_idx)                             \n",
    "            _, first_frame = cap.read()\n",
    "            first_frame = cv2.cvtColor(first_frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,  last_frame_idx)                             \n",
    "            _, last_frame = cap.read()\n",
    "            last_frame = cv2.cvtColor(last_frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if last_frame is None:\n",
    "                break\n",
    "        \n",
    "            fst_frame = self.data_transform(first_frame)\n",
    "            lst_frame = self.data_transform(last_frame)\n",
    "            inputs  = torch.stack([fst_frame, lst_frame])\n",
    "            \n",
    "            #    Calculate feature maps of the two frames\n",
    "            feat_maps = self.model(torch.autograd.Variable(inputs))\n",
    "            \n",
    "            # Measure distance\n",
    "            distance = self.metric(feat_maps[0], feat_maps[1])\n",
    "            \n",
    "            # Visualization\n",
    "            im1.set_data(first_frame)\n",
    "            im2.set_data(last_frame)\n",
    "            fig.canvas.draw()\n",
    "\n",
    "\n",
    "        plt.close(fig)\n",
    "        cap.release()\n",
    "\n",
    "    def _set_video_params(self, cap):\n",
    "    \n",
    "        params = {\n",
    "            'video_length': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "            'video_width' : int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            'video_height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            'video_fps'   : cap.get(cv2.CAP_PROP_FPS),\n",
    "        }\n",
    "        print(\"Video Duration: {:.3f} secs\".format( params['video_length']/params['video_fps']))\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from alexnet import alexnet_conv3, preprocessor\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "%matplotlib notebook\n",
    "\n",
    "VIDEO_FILE = '/tmp/%s.mp4'%title\n",
    "\n",
    "print(VIDEO_FILE)\n",
    "video_analyzer    = VideoAnalyzer(model          = alexnet_conv3(pretrained=True), \n",
    "                                  metric         = CosineSimilarity(),\n",
    "                                  data_transform = preprocessor())\n",
    "start = time.time()\n",
    "video_analyzer.process(VIDEO_FILE, sequence_len=50)\n",
    "print(\"Processed in {} secs\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "%matplotlib notebook\n",
    "\n",
    "img = cv2.cvtColor(cv2.imread('examples/cute_cat.jpg'), cv2.COLOR_BGR2RGB)\n",
    "frame_size = img.shape\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "fig.tight_layout()\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "\n",
    "im1 =  ax1.imshow(125*np.ones(frame_size))\n",
    "im2 =  ax2.imshow(125*np.ones(frame_size))\n",
    "\n",
    "fig.canvas.draw()\n",
    "b1 = fig.canvas.copy_from_bbox(ax1.bbox)\n",
    "b2 = fig.canvas.copy_from_bbox(ax2.bbox)\n",
    "print(ax2.images)\n",
    "\n",
    "start= time.time()\n",
    "for i in range(0,10):\n",
    "    img = np.random.randint(0, 255 , frame_size).astype(np.uint8)\n",
    "    \n",
    "    im1.set_data(img)\n",
    "    im2.set_data(img)\n",
    "    \n",
    "    fig.canvas.restore_region(b1)\n",
    "    fig.canvas.restore_region(b2)\n",
    "    \n",
    "    ax1.draw_artist(im1)\n",
    "    ax2.draw_artist(im2)\n",
    "\n",
    "    fig.canvas.blit(ax1.bbox)\n",
    "    fig.canvas.blit(ax2.bbox)\n",
    "#     fig.canvas.draw()\n",
    "    \n",
    "print(\"Completed in \", time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp3",
   "language": "python",
   "name": "webapp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
